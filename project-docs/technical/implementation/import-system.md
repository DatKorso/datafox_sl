# üì• –°–∏—Å—Ç–µ–º–∞ –∏–º–ø–æ—Ä—Ç–∞ –¥–∞–Ω–Ω—ã—Ö

> –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∏–º–ø–æ—Ä—Ç–∞ –¥–∞–Ω–Ω—ã—Ö –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–æ–≤

## üéØ –û–±–∑–æ—Ä —Å–∏—Å—Ç–µ–º—ã

–°–∏—Å—Ç–µ–º–∞ –∏–º–ø–æ—Ä—Ç–∞ DataFox SL –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–æ–≤ –≤ –µ–¥–∏–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö DuckDB —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π, —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –∏ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –∫–∞—á–µ—Å—Ç–≤–∞.

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏–º–ø–æ—Ä—Ç–∞

### –¢–∏–ø—ã –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö

#### **1. CSV —Ñ–∞–π–ª—ã (Ozon)**
```python
# –ó–∞–∫–∞–∑—ã Ozon
file_type: "csv"
read_params: {
    'delimiter': ';',
    'header': 0,
    'encoding': 'utf-8'
}
```

#### **2. XLSX —Ñ–∞–π–ª—ã (–æ–¥–∏–Ω–æ—á–Ω—ã–µ)**
```python
# –¶–µ–Ω—ã WB, —à—Ç—Ä–∏—Ö–∫–æ–¥—ã Ozon
file_type: "xlsx"
read_params: {
    'sheet_name': "–®—Ç—Ä–∏—Ö–∫–æ–¥—ã",
    'header': 2,
    'skip_rows_after_header': 1
}
```

#### **3. –ü–∞–ø–∫–∏ XLSX —Ñ–∞–π–ª–æ–≤**
```python
# –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ Ozon, —Ç–æ–≤–∞—Ä—ã WB
file_type: "folder_xlsx"
read_params: {
    'sheet_name': "–®–∞–±–ª–æ–Ω",
    'header': 1,
    'skip_rows_after_header': 2
}
```

#### **4. Google Sheets API**
```python
# –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ Punta
file_type: "google_sheets"
read_params: {
    'credentials_path': 'credentials.json',
    'range': 'A1:Z1000'
}
```

### –ü–æ—Ç–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö

–ü—Ä–æ—Ü–µ—Å—Å –∏–º–ø–æ—Ä—Ç–∞ –≤–∫–ª—é—á–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç—Ç–∞–ø–æ–≤:

1. **–ó–∞–≥—Ä—É–∑–∫–∞** - –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
2. **–í–∞–ª–∏–¥–∞—Ü–∏—è** - –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–µ–º—ã –∏ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö  
3. **–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è** - –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ü–µ–ª–µ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç
4. **–û—á–∏—Å—Ç–∫–∞** - –£–¥–∞–ª–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
5. **–í—Å—Ç–∞–≤–∫–∞** - –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –ë–î
6. **–í–∞–ª–∏–¥–∞—Ü–∏—è** - –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞

## üìù –ñ–µ—Å—Ç–∫–æ –∑–∞–¥–∞–Ω–Ω–∞—è —Å—Ö–µ–º–∞

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ HARDCODED_SCHEMA

```python
HARDCODED_SCHEMA = {
    "table_name": {
        "description": "–û–ø–∏—Å–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã",
        "file_type": "csv|xlsx|folder_xlsx|google_sheets",
        "read_params": {
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è pandas.read_*
        },
        "config_report_key": "key_in_config_json",
        "columns": [
            {
                'target_col_name': 'db_column_name',
                'sql_type': 'VARCHAR|INTEGER|DOUBLE|BIGINT|DATE',
                'source_col_name': 'Original Column Name',
                'notes': 'transformation_rules'
            }
        ],
        "pre_update_action": "DELETE FROM table_name;"
    }
}
```

### –ü—Ä–∏–º–µ—Ä—ã –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π

#### **Ozon Products (CSV)**
```python
"oz_products": {
    "description": "–¢–æ–≤–∞—Ä—ã Ozon",
    "file_type": "csv",
    "read_params": {
        'delimiter': ';', 
        'header': 0
    },
    "config_report_key": "oz_products_csv",
    "columns": [
        {
            'target_col_name': 'oz_vendor_code',
            'sql_type': 'VARCHAR',
            'source_col_name': '–ê—Ä—Ç–∏–∫—É–ª',
            'notes': "remove_single_quotes"
        },
        {
            'target_col_name': 'oz_sku',
            'sql_type': 'BIGINT',
            'source_col_name': 'SKU',
            'notes': None
        }
    ],
    "pre_update_action": "DELETE FROM oz_products;"
}
```

#### **WB Products (Folder XLSX)**
```python
"wb_products": {
    "description": "–¢–æ–≤–∞—Ä—ã Wildberries",
    "file_type": "folder_xlsx",
    "read_params": {
        'sheet_name': "–¢–æ–≤–∞—Ä—ã",
        'header': 2,
        'skip_rows_after_header': 1
    },
    "config_report_key": "wb_products_dir",
    "columns": [
        {
            'target_col_name': 'wb_sku',
            'sql_type': 'INTEGER',
            'source_col_name': '–ê—Ä—Ç–∏–∫—É–ª WB',
            'notes': None
        }
    ],
    "pre_update_action": "DELETE FROM wb_products;"
}
```

## üîß –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏–º–ø–æ—Ä—Ç–∞

### –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã

#### **1. –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ö–µ–º—ã**
```python
def validate_import_schema(df: pd.DataFrame, table_name: str) -> list[str]:
    """
    –í–∞–ª–∏–¥–∏—Ä—É–µ—Ç DataFrame –ø—Ä–æ—Ç–∏–≤ –∂–µ—Å—Ç–∫–æ –∑–∞–¥–∞–Ω–Ω–æ–π —Å—Ö–µ–º—ã
    
    Returns:
        list[str]: –°–ø–∏—Å–æ–∫ –æ—à–∏–±–æ–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    """
    schema = get_table_schema_definition(table_name)
    if not schema:
        return [f"–°—Ö–µ–º–∞ –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã {table_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"]
    
    errors = []
    required_columns = [col['source_col_name'] for col in schema['columns']]
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    missing_columns = set(required_columns) - set(df.columns)
    if missing_columns:
        errors.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
    for col_info in schema['columns']:
        source_col = col_info['source_col_name']
        sql_type = col_info['sql_type']
        
        if source_col in df.columns:
            validation_error = validate_column_type(df[source_col], sql_type)
            if validation_error:
                errors.append(f"{source_col}: {validation_error}")
    
    return errors
```

#### **2. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö**
```python
def transform_dataframe_for_import(df: pd.DataFrame, table_name: str) -> pd.DataFrame:
    """
    –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç DataFrame —Å–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–∞–≤–∏–ª–∞–º —Å—Ö–µ–º—ã
    """
    schema = get_table_schema_definition(table_name)
    transformed_df = df.copy()
    
    for col_info in schema['columns']:
        source_col = col_info['source_col_name']
        target_col = col_info['target_col_name']
        notes = col_info.get('notes')
        
        if source_col in transformed_df.columns:
            # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏
            if source_col != target_col:
                transformed_df.rename(columns={source_col: target_col}, inplace=True)
            
            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π
            if notes:
                transformed_df[target_col] = apply_transformation(
                    transformed_df[target_col], notes
                )
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫
    target_columns = [col['target_col_name'] for col in schema['columns']]
    extra_columns = set(transformed_df.columns) - set(target_columns)
    transformed_df.drop(columns=extra_columns, inplace=True)
    
    return transformed_df
```

#### **3. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ notes**
```python
def apply_transformation(series: pd.Series, transformation: str) -> pd.Series:
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –∫ —Å–µ—Ä–∏–∏ –¥–∞–Ω–Ω—ã—Ö
    """
    if transformation == "remove_single_quotes":
        return series.astype(str).str.replace("'", "")
    
    elif transformation == "convert_to_date":
        return pd.to_datetime(series, errors='coerce')
    
    elif transformation == "round_to_integer":
        return series.round().astype('Int64')
    
    elif transformation == "normalize_wb_barcodes":
        # –î–ª—è WB: "123;456;789" -> –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∑–∞–ø–∏—Å–∏
        return series.astype(str).str.strip()
    
    else:
        return series
```

### –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∑—á–∏–∫–∏

#### **CSV Loader**
```python
def load_csv_file(file_path: str, read_params: dict) -> pd.DataFrame:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ CSV —Ñ–∞–π–ª–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∫–æ–¥–∏—Ä–æ–≤–æ–∫
    """
    try:
        # –ü–æ–ø—ã—Ç–∫–∞ —Å UTF-8
        df = pd.read_csv(file_path, **read_params, encoding='utf-8')
    except UnicodeDecodeError:
        try:
            # Fallback –Ω–∞ Windows-1251
            df = pd.read_csv(file_path, **read_params, encoding='windows-1251')
        except UnicodeDecodeError:
            # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º
            df = pd.read_csv(file_path, **read_params, encoding='utf-8-sig')
    
    return df
```

#### **XLSX Loader**
```python
def load_xlsx_file(file_path: str, read_params: dict) -> pd.DataFrame:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ Excel —Ñ–∞–π–ª–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    """
    sheet_name = read_params.get('sheet_name', 0)
    header = read_params.get('header', 0)
    skip_rows = read_params.get('skip_rows_after_header', 0)
    
    # –ë–∞–∑–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
    df = pd.read_excel(file_path, sheet_name=sheet_name, header=header)
    
    # –ü—Ä–æ–ø—É—Å–∫ —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
    if skip_rows > 0:
        df = df.iloc[skip_rows:].reset_index(drop=True)
    
    # –û—á–∏—Å—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫
    df = df.dropna(how='all')
    
    return df
```

#### **Folder XLSX Loader**
```python
def load_folder_xlsx(folder_path: str, read_params: dict) -> pd.DataFrame:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö XLSX —Ñ–∞–π–ª–æ–≤ –∏–∑ –ø–∞–ø–∫–∏ —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ–º
    """
    xlsx_files = glob.glob(os.path.join(folder_path, "*.xlsx"))
    
    if not xlsx_files:
        raise ValueError(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ XLSX —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ: {folder_path}")
    
    dataframes = []
    
    for file_path in xlsx_files:
        try:
            df = load_xlsx_file(file_path, read_params)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ —Ñ–∞–π–ª–µ
            df['_source_file'] = os.path.basename(file_path)
            df['_import_date'] = datetime.now()
            
            dataframes.append(df)
            
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_path}: {e}")
            continue
    
    if not dataframes:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞")
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö DataFrame
    combined_df = pd.concat(dataframes, ignore_index=True)
    
    return combined_df
```

#### **Google Sheets Loader**
```python
def load_google_sheets(sheets_url: str, credentials_path: str) -> pd.DataFrame:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Google Sheets
    """
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Google Sheets API
        scope = ['https://spreadsheets.google.com/feeds',
                'https://www.googleapis.com/auth/drive']
        
        credentials = ServiceAccountCredentials.from_json_keyfile_name(
            credentials_path, scope
        )
        gc = gspread.authorize(credentials)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID —Ç–∞–±–ª–∏—Ü—ã –∏–∑ URL
        sheet_id = extract_sheet_id_from_url(sheets_url)
        sheet = gc.open_by_key(sheet_id)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–≤–æ–≥–æ –ª–∏—Å—Ç–∞
        worksheet = sheet.get_worksheet(0)
        data = worksheet.get_all_records()
        
        df = pd.DataFrame(data)
        
        # –û—á–∏—Å—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫ –∏ –∫–æ–ª–æ–Ω–æ–∫
        df = df.dropna(how='all').dropna(axis=1, how='all')
        
        return df
        
    except Exception as e:
        raise ConnectionError(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Google Sheets: {e}")
```

## üîÑ –ü—Ä–æ—Ü–µ—Å—Å –∏–º–ø–æ—Ä—Ç–∞

### –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏–º–ø–æ—Ä—Ç–∞

```python
def import_data_to_table(source_data, table_name: str, source_type: str) -> dict:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏–º–ø–æ—Ä—Ç–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–∞–±–ª–∏—Ü—É
    
    Args:
        source_data: –§–∞–π–ª, –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ –∏–ª–∏ URL
        table_name: –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –≤ —Å—Ö–µ–º–µ
        source_type: –¢–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        
    Returns:
        dict: –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–º–ø–æ—Ä—Ç–∞ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
    """
    result = {
        'success': False,
        'errors': [],
        'warnings': [],
        'imported_rows': 0,
        'processing_time': 0
    }
    
    start_time = time.time()
    
    try:
        # 1. –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ö–µ–º—ã
        schema = get_table_schema_definition(table_name)
        if not schema:
            result['errors'].append(f"–°—Ö–µ–º–∞ –¥–ª—è {table_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return result
        
        # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        if schema['file_type'] == 'csv':
            df = load_csv_file(source_data, schema['read_params'])
        elif schema['file_type'] == 'xlsx':
            df = load_xlsx_file(source_data, schema['read_params'])
        elif schema['file_type'] == 'folder_xlsx':
            df = load_folder_xlsx(source_data, schema['read_params'])
        elif schema['file_type'] == 'google_sheets':
            df = load_google_sheets(source_data, schema['read_params'])
        else:
            result['errors'].append(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞: {schema['file_type']}")
            return result
        
        # 3. –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ö–µ–º—ã
        validation_errors = validate_import_schema(df, table_name)
        if validation_errors:
            result['errors'].extend(validation_errors)
            return result
        
        # 4. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        transformed_df = transform_dataframe_for_import(df, table_name)
        
        # 5. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
        conn = connect_db()
        if not conn:
            result['errors'].append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
            return result
        
        # 6. Pre-update –¥–µ–π—Å—Ç–≤–∏–µ
        if schema.get('pre_update_action'):
            conn.execute(schema['pre_update_action'])
        
        # 7. –í—Å—Ç–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        success = insert_dataframe_to_table(conn, transformed_df, table_name)
        
        if success:
            result['success'] = True
            result['imported_rows'] = len(transformed_df)
        else:
            result['errors'].append("–û—à–∏–±–∫–∞ –≤—Å—Ç–∞–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–∞–±–ª–∏—Ü—É")
        
        conn.close()
        
    except Exception as e:
        result['errors'].append(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {str(e)}")
    
    finally:
        result['processing_time'] = time.time() - start_time
    
    return result
```

### Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞

```python
def import_multiple_files_batch(file_mappings: list[dict]) -> dict:
    """
    –ü–∞–∫–µ—Ç–Ω—ã–π –∏–º–ø–æ—Ä—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤
    
    Args:
        file_mappings: [
            {
                'file_path': '/path/to/file',
                'table_name': 'oz_products',
                'priority': 1
            }
        ]
    """
    results = {
        'total_files': len(file_mappings),
        'successful_imports': 0,
        'failed_imports': 0,
        'details': []
    }
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
    sorted_mappings = sorted(file_mappings, key=lambda x: x.get('priority', 999))
    
    for mapping in sorted_mappings:
        file_result = import_data_to_table(
            mapping['file_path'],
            mapping['table_name'],
            'auto'
        )
        
        results['details'].append({
            'file': mapping['file_path'],
            'table': mapping['table_name'],
            'result': file_result
        })
        
        if file_result['success']:
            results['successful_imports'] += 1
        else:
            results['failed_imports'] += 1
    
    return results
```

## üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

### –ú–µ—Ç—Ä–∏–∫–∏ –∏–º–ø–æ—Ä—Ç–∞

```python
def collect_import_metrics(result: dict, table_name: str) -> dict:
    """
    –°–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –∏–º–ø–æ—Ä—Ç–∞ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    """
    return {
        'timestamp': datetime.now().isoformat(),
        'table_name': table_name,
        'imported_rows': result.get('imported_rows', 0),
        'processing_time_seconds': result.get('processing_time', 0),
        'success': result.get('success', False),
        'error_count': len(result.get('errors', [])),
        'warning_count': len(result.get('warnings', [])),
        'throughput_rows_per_second': (
            result.get('imported_rows', 0) / max(result.get('processing_time', 1), 0.001)
        )
    }
```

### –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞

```python
import logging

def setup_import_logging():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –∏–º–ø–æ—Ä—Ç–∞"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('logs/import.log'),
            logging.StreamHandler()
        ]
    )

def log_import_process(table_name: str, result: dict):
    """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏–º–ø–æ—Ä—Ç–∞"""
    logger = logging.getLogger('DataFoxImport')
    
    if result['success']:
        logger.info(
            f"–£—Å–ø–µ—à–Ω—ã–π –∏–º–ø–æ—Ä—Ç –≤ {table_name}: "
            f"{result['imported_rows']} —Å—Ç—Ä–æ–∫ –∑–∞ {result['processing_time']:.2f}—Å"
        )
    else:
        logger.error(
            f"–û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –≤ {table_name}: {'; '.join(result['errors'])}"
        )
        
    if result.get('warnings'):
        logger.warning(
            f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ {table_name}: {'; '.join(result['warnings'])}"
        )
```

## üö® –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

### –¢–∏–ø—ã –æ—à–∏–±–æ–∫ –∏–º–ø–æ—Ä—Ç–∞

#### **1. –û—à–∏–±–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏**
```python
class SchemaValidationError(Exception):
    """–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å—Ö–µ–º—ã –¥–∞–Ω–Ω—ã—Ö"""
    pass

class DataTypeError(Exception):
    """–û—à–∏–±–∫–∞ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""
    pass
```

#### **2. –û—à–∏–±–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö**
```python
class DataSourceError(Exception):
    """–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –∏—Å—Ç–æ—á–Ω–∏–∫—É –¥–∞–Ω–Ω—ã—Ö"""
    pass

class FileFormatError(Exception):
    """–û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ —Ñ–∞–π–ª–∞"""
    pass
```

#### **3. –û—à–∏–±–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö**
```python
class DatabaseError(Exception):
    """–û—à–∏–±–∫–∞ —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö"""
    pass

class TableInsertError(Exception):
    """–û—à–∏–±–∫–∞ –≤—Å—Ç–∞–≤–∫–∏ –≤ —Ç–∞–±–ª–∏—Ü—É"""
    pass
```

### –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è

```python
def import_with_retry(source_data, table_name: str, max_retries: int = 3) -> dict:
    """
    –ò–º–ø–æ—Ä—Ç —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
    """
    for attempt in range(max_retries):
        try:
            result = import_data_to_table(source_data, table_name, 'auto')
            
            if result['success']:
                return result
            
            # –ü—Ä–∏ –Ω–µ—É—Å–ø–µ—Ö–µ –∂–¥–µ–º –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # Exponential backoff
                
        except Exception as e:
            if attempt == max_retries - 1:
                return {
                    'success': False,
                    'errors': [f"–í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã: {str(e)}"],
                    'imported_rows': 0
                }
            time.sleep(2 ** attempt)
    
    return {'success': False, 'errors': ['–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞'], 'imported_rows': 0}
```

## üîß –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–º–ø–æ—Ä—Ç–∞

### –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ config.json

```json
{
    "reports": {
        "oz_orders_csv": "/path/to/ozon/orders",
        "oz_products_csv": "/path/to/ozon/products.csv",
        "oz_barcodes_xlsx": "/path/to/ozon/barcodes.xlsx",
        "wb_products_dir": "/path/to/wb/products/",
        "wb_prices_xlsx": "/path/to/wb/prices.xlsx",
        "punta_google_sheets_url": "https://docs.google.com/spreadsheets/d/..."
    },
    "import_settings": {
        "batch_size": 1000,
        "max_retries": 3,
        "timeout_seconds": 300,
        "enable_logging": true,
        "backup_before_import": true
    }
}
```

### –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

```python
def validate_import_config() -> list[str]:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –∏–º–ø–æ—Ä—Ç–∞
    """
    errors = []
    config = load_config()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º
    for table_name in get_defined_table_names():
        schema = get_table_schema_definition(table_name)
        config_key = schema.get('config_report_key')
        
        if config_key:
            path = config.get('reports', {}).get(config_key)
            if not path:
                errors.append(f"–ù–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –ø—É—Ç—å –¥–ª—è {table_name}: {config_key}")
            elif schema['file_type'] in ['csv', 'xlsx'] and not os.path.exists(path):
                errors.append(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}")
            elif schema['file_type'] == 'folder_xlsx' and not os.path.isdir(path):
                errors.append(f"–ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {path}")
    
    return errors
```

---

## üìù –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ

**–ú–æ–¥—É–ª–∏**: `pages/2_üñá_–ò–º–ø–æ—Ä—Ç_–û—Ç—á–µ—Ç–æ–≤_–ú–ü.py`, `utils/data_cleaner.py`, `utils/db_schema.py`  
**–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ**: 2024-12-19  
**–í–µ—Ä—Å–∏—è**: 1.2.0  
**–°—Ç–∞—Ç—É—Å**: –°—Ç–∞–±–∏–ª—å–Ω—ã–π  

**–°–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã**:
- [–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã](../architecture-overview.md)
- [Database Utils API](../api/database-utils.md)
- [–°—Ö–µ–º–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö](../../data-structures/schemas/db_schema.md)

*–°–∏—Å—Ç–µ–º–∞ –∏–º–ø–æ—Ä—Ç–∞ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –Ω–∞–¥–µ–∂–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≤—Å–µ—Ö –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–æ–≤.* 