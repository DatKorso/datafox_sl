#!/usr/bin/env python3
"""
üÜò –≠–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è —É—Ç–∏–ª–∏—Ç–∞ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ Rich Content –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

–≠—Ç–∞ —É—Ç–∏–ª–∏—Ç–∞ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–∞ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ Rich Content –¥–∞–Ω–Ω—ã—Ö
–±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø–∞–º—è—Ç–∏ –±—Ä–∞—É–∑–µ—Ä–∞ –∏ WebSocket —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python utils/emergency_rich_content_export.py --help
    python utils/emergency_rich_content_export.py --all
    python utils/emergency_rich_content_export.py --brand "Nike" --output nike_rich_content.csv
    python utils/emergency_rich_content_export.py --limit 1000 --format json
"""

import argparse
import csv
import json
import logging
import sys
import time
from pathlib import Path
from typing import Optional, List, Dict, Any

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥ÔøΩÔøΩ–ª–µ–π
sys.path.append(str(Path(__file__).parent.parent))

from utils.db_connection import connect_db

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('emergency_export.log')
    ]
)
logger = logging.getLogger(__name__)


class EmergencyRichContentExporter:
    """–≠–∫—Å—Ç—Ä–µ–Ω–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä Rich Content –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, db_path: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä–∞
        
        Args:
            db_path: –ü—É—Ç—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ)
        """
        self.db_conn = connect_db(db_path) if db_path else connect_db()
        if not self.db_conn:
            raise ConnectionError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
        
        logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ Rich Content –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        logger.info("üìä –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ Rich Content...")
        
        try:
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_query = """
            SELECT COUNT(*) as total_products
            FROM oz_category_products
            """
            total_result = self.db_conn.execute(total_query).fetchone()
            total_products = total_result[0] if total_result else 0
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Rich Content
            rich_content_query = """
            SELECT 
                COUNT(*) as with_rich_content,
                COUNT(CASE WHEN LENGTH(rich_content_json) > 100 THEN 1 END) as valid_rich_content,
                AVG(LENGTH(rich_content_json)) as avg_size,
                MAX(LENGTH(rich_content_json)) as max_size,
                MIN(LENGTH(rich_content_json)) as min_size
            FROM oz_category_products
            WHERE rich_content_json IS NOT NULL 
            AND rich_content_json != ''
            """
            rich_result = self.db_conn.execute(rich_content_query).fetchone()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –±—Ä–µ–Ω–¥–∞–º
            brands_query = """
            SELECT 
                oz_brand,
                COUNT(*) as total,
                COUNT(CASE WHEN rich_content_json IS NOT NULL AND rich_content_json != '' 
                      THEN 1 END) as with_rich_content
            FROM oz_category_products
            WHERE oz_brand IS NOT NULL
            GROUP BY oz_brand
            HAVING with_rich_content > 0
            ORDER BY with_rich_content DESC
            LIMIT 10
            """
            brands_results = self.db_conn.execute(brands_query).fetchall()
            
            stats = {
                'total_products': total_products,
                'with_rich_content': rich_result[0] if rich_result else 0,
                'valid_rich_content': rich_result[1] if rich_result else 0,
                'avg_size_bytes': int(rich_result[2]) if rich_result and rich_result[2] else 0,
                'max_size_bytes': rich_result[3] if rich_result else 0,
                'min_size_bytes': rich_result[4] if rich_result else 0,
                'coverage_percent': round((rich_result[0] / total_products * 100), 2) if total_products > 0 else 0,
                'top_brands': [
                    {
                        'brand': row[0],
                        'total': row[1],
                        'with_rich_content': row[2],
                        'coverage_percent': round((row[2] / row[1] * 100), 2)
                    }
                    for row in brands_results
                ]
            }
            
            logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–ª—É—á–µ–Ω–∞: {stats['with_rich_content']} —Ç–æ–≤–∞—Ä–æ–≤ —Å Rich Content")
            return stats
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {}
    
    def export_all(
        self, 
        output_file: str = None, 
        format: str = 'csv',
        chunk_size: int = 1000
    ) -> bool:
        """
        –≠–∫—Å–ø–æ—Ä—Ç –≤—Å–µ—Ö Rich Content –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            output_file: –ò–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            format: –§–æ—Ä–º–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞ ('csv' –∏–ª–∏ 'json')
            chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            
        Returns:
            True –µ—Å–ª–∏ —ç–∫—Å–ø–æ—Ä—Ç —É—Å–ø–µ—à–µ–Ω
        """
        if not output_file:
            timestamp = int(time.time())
            output_file = f"rich_content_full_export_{timestamp}.{format}"
        
        logger.info(f"üì• –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–ª–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç Rich Content –≤ —Ñ–∞–π–ª: {output_file}")
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π
            count_query = """
            SELECT COUNT(*)
            FROM oz_category_products ocp
            LEFT JOIN oz_products op ON ocp.oz_vendor_code = op.oz_vendor_code
            WHERE ocp.rich_content_json IS NOT NULL 
            AND ocp.rich_content_json != ''
            AND LENGTH(ocp.rich_content_json) > 10
            """
            total_count = self.db_conn.execute(count_query).fetchone()[0]
            logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {total_count} –∑–∞–ø–∏—Å–µ–π –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            
            if total_count == 0:
                logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
                return False
            
            # –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π
            base_query = """
            SELECT 
                ocp.oz_vendor_code,
                op.oz_sku,
                ocp.product_name,
                ocp.type,
                ocp.gender,
                ocp.oz_brand,
                ocp.russian_size,
                ocp.season,
                ocp.color,
                op.oz_fbo_stock,
                ocp.rich_content_json,
                LENGTH(ocp.rich_content_json) as json_size
            FROM oz_category_products ocp
            LEFT JOIN oz_products op ON ocp.oz_vendor_code = op.oz_vendor_code
            WHERE ocp.rich_content_json IS NOT NULL 
            AND ocp.rich_content_json != ''
            AND LENGTH(ocp.rich_content_json) > 10
            ORDER BY ocp.oz_vendor_code
            LIMIT ? OFFSET ?
            """
            
            exported_count = 0
            
            if format == 'csv':
                with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
                    fieldnames = [
                        'oz_vendor_code', 'oz_sku', 'product_name', 'type', 'gender',
                        'oz_brand', 'russian_size', 'season', 'color', 'oz_fbo_stock',
                        'rich_content_json', 'json_size_bytes'
                    ]
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writeheader()
                    
                    # –≠–∫—Å–ø–æ—Ä—Ç –ø–æ —á–∞–Ω–∫–∞–º
                    offset = 0
                    while offset < total_count:
                        logger.info(f"‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ {offset + 1}-{min(offset + chunk_size, total_count)} –∏–∑ {total_count}")
                        
                        results = self.db_conn.execute(base_query, [chunk_size, offset]).fetchall()
                        
                        for row in results:
                            writer.writerow({
                                'oz_vendor_code': row[0],
                                'oz_sku': row[1] or '',
                                'product_name': row[2] or '',
                                'type': row[3] or '',
                                'gender': row[4] or '',
                                'oz_brand': row[5] or '',
                                'russian_size': row[6] or '',
                                'season': row[7] or '',
                                'color': row[8] or '',
                                'oz_fbo_stock': row[9] or 0,
                                'rich_content_json': row[10],
                                'json_size_bytes': row[11]
                            })
                            exported_count += 1
                        
                        offset += chunk_size
            
            elif format == 'json':
                all_data = []
                
                # –≠–∫—Å–ø–æ—Ä—Ç –ø–æ —á–∞–Ω–∫–∞–º
                offset = 0
                while offset < total_count:
                    logger.info(f"‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ {offset + 1}-{min(offset + chunk_size, total_count)} –∏–∑ {total_count}")
                    
                    results = self.db_conn.execute(base_query, [chunk_size, offset]).fetchall()
                    
                    for row in results:
                        # –ü–∞—Ä—Å–∏–º Rich Content JSON –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                        try:
                            rich_content_data = json.loads(row[10])
                        except json.JSONDecodeError:
                            logger.warning(f"‚ö†Ô∏è –ù–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON –¥–ª—è —Ç–æ–≤–∞—Ä–∞ {row[0]}")
                            rich_content_data = {"error": "Invalid JSON"}
                        
                        all_data.append({
                            'oz_vendor_code': row[0],
                            'oz_sku': row[1],
                            'product_name': row[2],
                            'type': row[3],
                            'gender': row[4],
                            'oz_brand': row[5],
                            'russian_size': row[6],
                            'season': row[7],
                            'color': row[8],
                            'oz_fbo_stock': row[9],
                            'rich_content_json': rich_content_data,
                            'json_size_bytes': row[11]
                        })
                        exported_count += 1
                    
                    offset += chunk_size
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON —Ñ–∞–π–ª
                with open(output_file, 'w', encoding='utf-8') as jsonfile:
                    json.dump(all_data, jsonfile, ensure_ascii=False, indent=2)
            
            logger.info(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: {exported_count} –∑–∞–ø–∏—Å–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {output_file}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            file_size = Path(output_file).stat().st_size
            file_size_mb = file_size / (1024 * 1024)
            logger.info(f"üìä –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size_mb:.2f} –ú–ë")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {e}")
            return False
    
    def export_by_brand(
        self, 
        brand: str, 
        output_file: str = None, 
        format: str = 'csv'
    ) -> bool:
        """
        –≠–∫—Å–ø–æ—Ä—Ç Rich Content –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –±—Ä–µ–Ω–¥–∞
        
        Args:
            brand: –ù–∞–∑–≤–∞–Ω–∏–µ –±—Ä–µ–Ω–¥–∞
            output_file: –ò–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            format: –§–æ—Ä–º–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞
            
        Returns:
            True –µ—Å–ª–∏ —ç–∫—Å–ø–æ—Ä—Ç —É—Å–ø–µ—à–µ–Ω
        """
        if not output_file:
            safe_brand = "".join(c for c in brand if c.isalnum() or c in (' ', '-', '_')).rstrip()
            timestamp = int(time.time())
            output_file = f"rich_content_{safe_brand}_{timestamp}.{format}"
        
        logger.info(f"üì• –≠–∫—Å–ø–æ—Ä—Ç Rich Content –¥–ª—è –±—Ä–µ–Ω–¥–∞ '{brand}' –≤ —Ñ–∞–π–ª: {output_file}")
        
        try:
            query = """
            SELECT 
                ocp.oz_vendor_code,
                op.oz_sku,
                ocp.product_name,
                ocp.type,
                ocp.gender,
                ocp.oz_brand,
                ocp.russian_size,
                ocp.season,
                ocp.color,
                op.oz_fbo_stock,
                ocp.rich_content_json,
                LENGTH(ocp.rich_content_json) as json_size
            FROM oz_category_products ocp
            LEFT JOIN oz_products op ON ocp.oz_vendor_code = op.oz_vendor_code
            WHERE ocp.oz_brand = ?
            AND ocp.rich_content_json IS NOT NULL 
            AND ocp.rich_content_json != ''
            AND LENGTH(ocp.rich_content_json) > 10
            ORDER BY ocp.oz_vendor_code
            """
            
            results = self.db_conn.execute(query, [brand]).fetchall()
            
            if not results:
                logger.warning(f"‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö Rich Content –¥–ª—è –±—Ä–µ–Ω–¥–∞ '{brand}'")
                return False
            
            logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ç–æÔøΩÔøΩ–∞—Ä–æ–≤ —Å Rich Content –¥–ª—è –±—Ä–µ–Ω–¥–∞ '{brand}'")
            
            if format == 'csv':
                with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
                    fieldnames = [
                        'oz_vendor_code', 'oz_sku', 'product_name', 'type', 'gender',
                        'oz_brand', 'russian_size', 'season', 'color', 'oz_fbo_stock',
                        'rich_content_json', 'json_size_bytes'
                    ]
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writeheader()
                    
                    for row in results:
                        writer.writerow({
                            'oz_vendor_code': row[0],
                            'oz_sku': row[1] or '',
                            'product_name': row[2] or '',
                            'type': row[3] or '',
                            'gender': row[4] or '',
                            'oz_brand': row[5] or '',
                            'russian_size': row[6] or '',
                            'season': row[7] or '',
                            'color': row[8] or '',
                            'oz_fbo_stock': row[9] or 0,
                            'rich_content_json': row[10],
                            'json_size_bytes': row[11]
                        })
            
            elif format == 'json':
                data = []
                for row in results:
                    try:
                        rich_content_data = json.loads(row[10])
                    except json.JSONDecodeError:
                        logger.warning(f"‚ö†Ô∏è –ù–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON –¥–ª—è —Ç–æ–≤–∞—Ä–∞ {row[0]}")
                        rich_content_data = {"error": "Invalid JSON"}
                    
                    data.append({
                        'oz_vendor_code': row[0],
                        'oz_sku': row[1],
                        'product_name': row[2],
                        'type': row[3],
                        'gender': row[4],
                        'oz_brand': row[5],
                        'russian_size': row[6],
                        'season': row[7],
                        'color': row[8],
                        'oz_fbo_stock': row[9],
                        'rich_content_json': rich_content_data,
                        'json_size_bytes': row[11]
                    })
                
                with open(output_file, 'w', encoding='utf-8') as jsonfile:
                    json.dump(data, jsonfile, ensure_ascii=False, indent=2)
            
            logger.info(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: {len(results)} –∑–∞–ø–∏—Å–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {output_file}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –¥–ª—è –±—Ä–µ–Ω–¥–∞ '{brand}': {e}")
            return False
    
    def export_limited(
        self, 
        limit: int, 
        output_file: str = None, 
        format: str = 'csv',
        order_by: str = 'oz_vendor_code'
    ) -> bool:
        """
        –≠–∫—Å–ø–æ—Ä—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø–∏—Å–µ–π
        
        Args:
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π
            output_file: –ò–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            format: –§–æ—Ä–º–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞
            order_by: –ü–æ–ª–µ –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
            
        Returns:
            True –µ—Å–ª–∏ —ç–∫—Å–ø–æ—Ä—Ç —É—Å–ø–µ—à–µ–Ω
        """
        if not output_file:
            timestamp = int(time.time())
            output_file = f"rich_content_limited_{limit}_{timestamp}.{format}"
        
        logger.info(f"üì• –≠–∫—Å–ø–æ—Ä—Ç {limit} –∑–∞–ø–∏—Å–µ–π Rich Content –≤ —Ñ–∞–π–ª: {output_file}")
        
        try:
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
            valid_order_fields = [
                'oz_vendor_code', 'product_name', 'oz_brand', 'type', 
                'oz_fbo_stock', 'json_size'
            ]
            if order_by not in valid_order_fields:
                order_by = 'oz_vendor_code'
                logger.warning(f"‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω–æ–µ –ø–æ–ª–µ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 'oz_vendor_code'")
            
            query = f"""
            SELECT 
                ocp.oz_vendor_code,
                op.oz_sku,
                ocp.product_name,
                ocp.type,
                ocp.gender,
                ocp.oz_brand,
                ocp.russian_size,
                ocp.season,
                ocp.color,
                op.oz_fbo_stock,
                ocp.rich_content_json,
                LENGTH(ocp.rich_content_json) as json_size
            FROM oz_category_products ocp
            LEFT JOIN oz_products op ON ocp.oz_vendor_code = op.oz_vendor_code
            WHERE ocp.rich_content_json IS NOT NULL 
            AND ocp.rich_content_json != ''
            AND LENGTH(ocp.rich_content_json) > 10
            ORDER BY {order_by}
            LIMIT ?
            """
            
            results = self.db_conn.execute(query, [limit]).fetchall()
            
            if not results:
                logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
                return False
            
            logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(results)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            
            if format == 'csv':
                with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
                    fieldnames = [
                        'oz_vendor_code', 'oz_sku', 'product_name', 'type', 'gender',
                        'oz_brand', 'russian_size', 'season', 'color', 'oz_fbo_stock',
                        'rich_content_json', 'json_size_bytes'
                    ]
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writeheader()
                    
                    for row in results:
                        writer.writerow({
                            'oz_vendor_code': row[0],
                            'oz_sku': row[1] or '',
                            'product_name': row[2] or '',
                            'type': row[3] or '',
                            'gender': row[4] or '',
                            'oz_brand': row[5] or '',
                            'russian_size': row[6] or '',
                            'season': row[7] or '',
                            'color': row[8] or '',
                            'oz_fbo_stock': row[9] or 0,
                            'rich_content_json': row[10],
                            'json_size_bytes': row[11]
                        })
            
            elif format == 'json':
                data = []
                for row in results:
                    try:
                        rich_content_data = json.loads(row[10])
                    except json.JSONDecodeError:
                        logger.warning(f"‚ö†Ô∏è –ù–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON –¥–ª—è —Ç–æ–≤–∞—Ä–∞ {row[0]}")
                        rich_content_data = {"error": "Invalid JSON"}
                    
                    data.append({
                        'oz_vendor_code': row[0],
                        'oz_sku': row[1],
                        'product_name': row[2],
                        'type': row[3],
                        'gender': row[4],
                        'oz_brand': row[5],
                        'russian_size': row[6],
                        'season': row[7],
                        'color': row[8],
                        'oz_fbo_stock': row[9],
                        'rich_content_json': rich_content_data,
                        'json_size_bytes': row[11]
                    })
                
                with open(output_file, 'w', encoding='utf-8') as jsonfile:
                    json.dump(data, jsonfile, ensure_ascii=False, indent=2)
            
            logger.info(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: {len(results)} –∑–∞–ø–∏—Å–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {output_file}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ —ç–∫—Å–ø–æ—Ä—Ç–∞: {e}")
            return False
    
    def validate_rich_content(self, limit: int = 100) -> Dict[str, Any]:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è Rich Content JSON –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        logger.info(f"üîç –í–∞–ª–∏–¥–∞—Ü–∏—è {limit} –∑–∞–ø–∏—Å–µ–π Rich Content...")
        
        try:
            query = """
            SELECT oz_vendor_code, rich_content_json
            FROM oz_category_products
            WHERE rich_content_json IS NOT NULL 
            AND rich_content_json != ''
            LIMIT ?
            """
            
            results = self.db_conn.execute(query, [limit]).fetchall()
            
            validation_stats = {
                'total_checked': len(results),
                'valid_json': 0,
                'invalid_json': 0,
                'valid_structure': 0,
                'invalid_structure': 0,
                'errors': []
            }
            
            for vendor_code, rich_content_json in results:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ JSON
                try:
                    data = json.loads(rich_content_json)
                    validation_stats['valid_json'] += 1
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã Rich Content
                    if isinstance(data, dict) and 'content' in data and 'version' in data:
                        if isinstance(data['content'], list):
                            validation_stats['valid_structure'] += 1
                        else:
                            validation_stats['invalid_structure'] += 1
                            validation_stats['errors'].append(f"{vendor_code}: content –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –º–∞—Å—Å–∏–≤–æ–º")
                    else:
                        validation_stats['invalid_structure'] += 1
                        validation_stats['errors'].append(f"{vendor_code}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è")
                        
                except json.JSONDecodeError as e:
                    validation_stats['invalid_json'] += 1
                    validation_stats['errors'].append(f"{vendor_code}: JSON –æ—à–∏–±–∫–∞ - {str(e)}")
            
            logger.info(f"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {validation_stats['valid_structure']}/{validation_stats['total_checked']} –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π")
            return validation_stats
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
            return {}
    
    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö"""
        if self.db_conn:
            self.db_conn.close()
            logger.info("üîå –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö –∑–∞–∫—Ä—ã—Ç–æ")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è CLI"""
    parser = argparse.ArgumentParser(
        description="üÜò –≠–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è —É—Ç–∏–ª–∏—Ç–∞ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ Rich Content",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

  # –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
  python utils/emergency_rich_content_export.py --stats

  # –≠–∫—Å–ø–æ—Ä—Ç –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –≤ CSV
  python utils/emergency_rich_content_export.py --all

  # –≠–∫—Å–ø–æ—Ä—Ç –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –≤ JSON
  python utils/emergency_rich_content_export.py --all --format json

  # –≠–∫—Å–ø–æ—Ä—Ç –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –±—Ä–µ–Ω–¥–∞
  python utils/emergency_rich_content_export.py --brand "Nike" --output nike_data.csv

  # –≠–∫—Å–ø–æ—Ä—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø–∏—Å–µ–π
  python utils/emergency_rich_content_export.py --limit 1000 --order-by oz_fbo_stock

  # –í–∞–ª–∏–¥–∞—Ü–∏—è Rich Content JSON
  python utils/emergency_rich_content_export.py --validate 500
        """
    )
    
    parser.add_argument('--stats', action='store_true', help='–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É Rich Content')
    parser.add_argument('--all', action='store_true', help='–≠–∫—Å–ø–æ—Ä—Ç –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö')
    parser.add_argument('--brand', type=str, help='–≠–∫—Å–ø–æ—Ä—Ç –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –±—Ä–µ–Ω–¥–∞')
    parser.add_argument('--limit', type=int, help='–û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π')
    parser.add_argument('--output', type=str, help='–ò–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞')
    parser.add_argument('--format', choices=['csv', 'json'], default='csv', help='–§–æ—Ä–º–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞')
    parser.add_argument('--order-by', type=str, default='oz_vendor_code', help='–ü–æ–ª–µ –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏')
    parser.add_argument('--chunk-size', type=int, default=1000, help='–†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏')
    parser.add_argument('--validate', type=int, help='–í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å N –∑–∞–ø–∏—Å–µ–π Rich Content JSON')
    parser.add_argument('--db-path', type=str, help='–ü—É—Ç—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö')
    
    args = parser.parse_args()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —É–∫–∞–∑–∞–Ω–∞ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ –æ–ø–µ—Ä–∞—Ü–∏—è
    if not any([args.stats, args.all, args.brand, args.limit, args.validate]):
        parser.print_help()
        return
    
    try:
        # –°–æ–∑–¥–∞–µ–º —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä
        exporter = EmergencyRichContentExporter(args.db_path)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –æ–ø–µ—Ä–∞—Ü–∏–∏
        if args.stats:
            print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê RICH CONTENT")
            print("=" * 50)
            stats = exporter.get_statistics()
            
            if stats:
                print(f"–í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤: {stats['total_products']:,}")
                print(f"–° Rich Content: {stats['with_rich_content']:,}")
                print(f"–í–∞–ª–∏–¥–Ω—ã–π Rich Content: {stats['valid_rich_content']:,}")
                print(f"–ü–æ–∫—Ä—ã—Ç–∏–µ: {stats['coverage_percent']}%")
                print(f"–°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä JSON: {stats['avg_size_bytes']:,} –±–∞–π—Ç")
                print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {stats['max_size_bytes']:,} –±–∞–π—Ç")
                
                if stats['top_brands']:
                    print("\n–¢–û–ü-10 –±—ÄÔøΩÔøΩ–Ω–¥–æ–≤ —Å Rich Content:")
                    for i, brand in enumerate(stats['top_brands'], 1):
                        print(f"{i:2d}. {brand['brand']:<20} {brand['with_rich_content']:>6,} —Ç–æ–≤–∞—Ä–æ–≤ ({brand['coverage_percent']:>5.1f}%)")
        
        if args.validate:
            print(f"\nüîç –í–ê–õ–ò–î–ê–¶–ò–Ø {args.validate} –ó–ê–ü–ò–°–ï–ô")
            print("=" * 50)
            validation_stats = exporter.validate_rich_content(args.validate)
            
            if validation_stats:
                print(f"–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {validation_stats['total_checked']}")
                print(f"–í–∞–ª–∏–¥–Ω—ã–π JSON: {validation_stats['valid_json']}")
                print(f"–ù–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON: {validation_stats['invalid_json']}")
                print(f"–ö–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: {validation_stats['valid_structure']}")
                print(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: {validation_stats['invalid_structure']}")
                
                if validation_stats['errors']:
                    print(f"\n–ü–µ—Ä–≤—ã–µ 10 –æ—à–∏–±–æ–∫:")
                    for error in validation_stats['errors'][:10]:
                        print(f"  ‚ùå {error}")
        
        if args.all:
            print(f"\nüì• –ü–û–õ–ù–´–ô –≠–ö–°–ü–û–†–¢ –í –§–û–†–ú–ê–¢–ï {args.format.upper()}")
            print("=" * 50)
            success = exporter.export_all(args.output, args.format, args.chunk_size)
            if success:
                print("‚úÖ –ü–æ–ª–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            else:
                print("‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª–Ω–æ–≥–æ —ç–∫—Å–ø–æ—Ä—Ç–∞")
        
        if args.brand:
            print(f"\nüì• –≠–ö–°–ü–û–†–¢ –ë–†–ï–ù–î–ê '{args.brand}' –í –§–û–†–ú–ê–¢–ï {args.format.upper()}")
            print("=" * 50)
            success = exporter.export_by_brand(args.brand, args.output, args.format)
            if success:
                print(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç –±—Ä–µ–Ω–¥–∞ '{args.brand}' –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –±—Ä–µ–Ω–¥–∞ '{args.brand}'")
        
        if args.limit:
            print(f"\nüì• –û–ì–†–ê–ùÔøΩÔøΩ–ß–ï–ù–ù–´–ô –≠–ö–°–ü–û–†–¢ ({args.limit} –∑–∞–ø–∏—Å–µ–π) –í –§–û–†–ú–ê–¢–ï {args.format.upper()}")
            print("=" * 50)
            success = exporter.export_limited(args.limit, args.output, args.format, args.order_by)
            if success:
                print(f"‚úÖ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç ({args.limit} –∑–∞–ø–∏—Å–µ–π) –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ —ç–∫—Å–ø–æ—Ä—Ç–∞")
        
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
        exporter.close()
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()