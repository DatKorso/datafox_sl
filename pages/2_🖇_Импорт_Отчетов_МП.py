"""
Streamlit page for importing marketplace reports into the database.

This page allows users to:
- Select a marketplace (Ozon or Wildberries).
- Provide report files (or paths to files/folders) corresponding to the selected marketplace.
  - Users can upload files directly or use pre-configured paths from `config.json`.
- Trigger an import process that reads the files, transforms data according to `mp_reports_schema.md`,
  and loads it into the DuckDB database.
- View feedback on the success or failure of each import operation.
"""
import streamlit as st
import pandas as pd
import os
from utils.config_utils import load_config, get_report_path, get_db_path
from utils.db_connection import connect_db, get_connection_and_ensure_schema
from utils.db_crud import import_data_from_dataframe
from utils.db_schema import get_table_schema_definition, get_defined_table_names, create_tables_from_schema
from utils.google_sheets_utils import read_google_sheets_as_dataframe, validate_google_sheets_url, test_google_sheets_access

# (Keep other imports like ui_utils if they exist)
# from utils.ui_utils import show_navigation_links

# Initialize session state for selected marketplace if not already done
if 'selected_marketplace' not in st.session_state:
    st.session_state.selected_marketplace = "Ozon" # Default selection

# Load configuration
config = load_config()
# Ensure db_path is loaded correctly for use in this script if needed for connection status
db_path = get_db_path()

st.set_page_config(layout="wide", page_title="–ò–º–ø–æ—Ä—Ç –æ—Ç—á–µ—Ç–æ–≤ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–æ–≤")
# show_navigation_links() # If using ui_utils
st.title("üõí –ò–º–ø–æ—Ä—Ç –æ—Ç—á–µ—Ç–æ–≤ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–æ–≤")
st.write("–ù–∞ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –≤—ã –º–æ–∂–µ—Ç–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –æ—Ç—á–µ—Ç—ã —Å –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–æ–≤ Ozon –∏ Wildberries.")

# --- Helper Function to get connection and ensure schema ---
db_conn = get_connection_and_ensure_schema()

if not db_conn:
    st.warning("–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö. –ò–º–ø–æ—Ä—Ç –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω.")
    st.error("–ü—É—Ç—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏.")
    if st.button("–ü–µ—Ä–µ–π—Ç–∏ –≤ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ë–î"):
        st.switch_page("pages/3_Settings.py")
    st.stop()
else:
    # Check if schema creation was successful
    if not create_tables_from_schema(db_conn):
        st.warning("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏/–ø—Ä–æ–≤–µ—Ä–∫–µ —Ç–∞–±–ª–∏—Ü –≤ –ë–î. –ò–º–ø–æ—Ä—Ç –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ.")
    
    # Check if database migration is needed
    from utils.db_migration import auto_migrate_if_needed
    if not auto_migrate_if_needed(db_conn):
        st.warning("‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç—Å—è –≤—ã–ø–æ–ª–Ω–∏—Ç—å –º–∏–≥—Ä–∞—Ü–∏—é –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –∏–º–ø–æ—Ä—Ç–æ–º.")
        st.info("–í—ã–ø–æ–ª–Ω–∏—Ç–µ –º–∏–≥—Ä–∞—Ü–∏—é –≤—ã—à–µ, –∑–∞—Ç–µ–º –æ–±–Ω–æ–≤–∏—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –∏–º–ø–æ—Ä—Ç–∞.")
        st.stop()

# --- Marketplace and Report Selection ---
st.sidebar.title("–í—ã–±–æ—Ä –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–∞")
mp_options = ["Ozon", "Wildberries"]
st.session_state.selected_marketplace = st.sidebar.radio(
    "–í—ã–±–µ—Ä–∏—Ç–µ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å:",
    mp_options,
    index=mp_options.index(st.session_state.selected_marketplace) # Maintain selection
)

# Get defined tables for the selected marketplace from schema
defined_tables = get_defined_table_names() # This gets ALL table names
# We need to filter them by marketplace, or adjust schema to include marketplace info
# For now, let's assume get_table_schema_definition gives us enough info to filter.

marketplace_specific_tables = {}
for table_name in defined_tables:
    schema_def = get_table_schema_definition(table_name)
    if schema_def:
        # Infer marketplace from table name prefix or a new 'marketplace' key in schema
        # Simple inference for now:
        if st.session_state.selected_marketplace == "Ozon" and table_name.startswith("oz_"):
            marketplace_specific_tables[table_name] = schema_def.get("description", table_name)
        elif st.session_state.selected_marketplace == "Wildberries" and table_name.startswith("wb_"):
            marketplace_specific_tables[table_name] = schema_def.get("description", table_name)

if not marketplace_specific_tables:
    st.sidebar.warning(f"–î–ª—è {st.session_state.selected_marketplace} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –æ—Ç—á–µ—Ç–æ–≤ –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞.")
else:
    st.sidebar.subheader(f"–û—Ç—á–µ—Ç—ã –¥–ª—è {st.session_state.selected_marketplace}")
    selected_report_key = st.sidebar.selectbox(
        "–í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø –æ—Ç—á–µ—Ç–∞ –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞:",
        options=list(marketplace_specific_tables.keys()),
        format_func=lambda x: marketplace_specific_tables[x]
    )
    
    st.subheader(f"–ò–º–ø–æ—Ä—Ç: {marketplace_specific_tables.get(selected_report_key, '')} ({st.session_state.selected_marketplace})")

    schema_for_selected_report = get_table_schema_definition(selected_report_key)
    
    if schema_for_selected_report:
        config_key_for_path = schema_for_selected_report.get("config_report_key")
        default_path = get_report_path(config_key_for_path) if config_key_for_path else ""
        file_type = schema_for_selected_report.get("file_type", "file") # "file" or "folder_xlsx" or "google_sheets"
        
        uploaded_files = None # Initialize to None
        data_source_path = None # Initialize
        use_config_path = False

        if file_type == "google_sheets":
            st.info("–î–ª—è —ç—Ç–æ–≥–æ —Ç–∏–ø–∞ –æ—Ç—á–µ—Ç–∞ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ —Å—Å—ã–ª–∫–∞ –Ω–∞ Google Sheets –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫.")
            sheets_url = default_path
            
            if sheets_url:
                if validate_google_sheets_url(sheets_url):
                    st.success(f"‚úÖ Google Sheets URL –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {sheets_url[:50]}...")
                    if st.button("üîó –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–æ—Å—Ç—É–ø –∫ Google Sheets", key=f"test_sheets_{selected_report_key}"):
                        with st.spinner("–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ Google Sheets..."):
                            if test_google_sheets_access(sheets_url):
                                st.success("‚úÖ Google Sheets –¥–æ–∫—É–º–µ–Ω—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞")
                            else:
                                st.error("‚ùå Google Sheets –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Å—ã–ª–∫—É –∏ –ø—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞.")
                else:
                    st.error("‚ùå –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ Google Sheets. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ URL –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö.")
            else:
                st.warning("‚ö†Ô∏è Google Sheets URL –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.")
                if st.button("–ü–µ—Ä–µ–π—Ç–∏ –≤ –ù–∞—Å—Ç—Ä–æ–π–∫–∏", key=f"goto_settings_{selected_report_key}"):
                    st.switch_page("pages/3_Settings.py")
            
            data_source_path = sheets_url
            
        elif file_type == "folder_xlsx":
            st.info("–î–ª—è —ç—Ç–æ–≥–æ —Ç–∏–ø–∞ –æ—Ç—á–µ—Ç–∞ –æ–∂–∏–¥–∞–µ—Ç—Å—è –ø–∞–ø–∫–∞ —Å XLSX —Ñ–∞–π–ª–∞–º–∏.")
            folder_path_input = st.text_input("–ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å —Ñ–∞–π–ª–∞–º–∏ XLSX:", value=default_path, key=f"folder_path_{selected_report_key}")
            if folder_path_input and os.path.isdir(folder_path_input):
                try:
                    if not any(f.endswith(".xlsx") for f in os.listdir(folder_path_input)):
                        st.warning("–í —É–∫–∞–∑–∞–Ω–Ω–æ–π –ø–∞–ø–∫–µ –Ω–µ—Ç XLSX —Ñ–∞–π–ª–æ–≤.")
                except Exception as e:
                    st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –ø–∞–ø–∫—É: {e}")
            elif folder_path_input: # Path provided but not a directory
                 st.warning("–£–∫–∞–∑–∞–Ω–Ω—ã–π –ø—É—Ç—å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø–∞–ø–∫–æ–π –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")
            data_source_path = folder_path_input
            
        else: # Single file upload
            accept_multiple = schema_for_selected_report.get("accept_multiple_files", False) # Get from schema or default to False
            file_label = f"–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –æ—Ç—á–µ—Ç–∞ ({schema_for_selected_report.get('file_type_description', schema_for_selected_report.get('file_type', ''))})" # Improved label

            if default_path and os.path.exists(default_path): # Check if default_path is valid and exists
                use_config_path = st.checkbox(
                    f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—É—Ç—å –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫: {default_path}", 
                    value=True, # Default to using config path if available
                    key=f"use_config_path_{selected_report_key}"
                )
            
            if use_config_path and default_path:
                data_source_path = default_path
                st.caption(f"–ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω —Ñ–∞–π–ª: {default_path}")
                # Optionally disable or hide file_uploader when using config path
                # For simplicity, we'll just ignore uploaded_files if use_config_path is True
            else:
                uploaded_files = st.file_uploader(
                    file_label,
                    type=[schema_for_selected_report.get("file_type")] if schema_for_selected_report.get("file_type") not in ["csv", "xlsx"] else ["csv", "xlsx"],
                    accept_multiple_files=accept_multiple,
                    key=f"uploader_{selected_report_key}"
                )

        if st.button(f"–ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å {marketplace_specific_tables.get(selected_report_key, '')}"):
            proceed_with_import = False
            if file_type == "google_sheets":
                if data_source_path and validate_google_sheets_url(data_source_path):
                    proceed_with_import = True
                else:
                    st.warning("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ Google Sheets.")
            elif file_type == "folder_xlsx":
                if data_source_path and os.path.isdir(data_source_path):
                    proceed_with_import = True
                else:
                    st.warning("–£–∫–∞–∂–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—É—é –ø–∞–ø–∫—É —Å —Ñ–∞–π–ª–∞–º–∏ XLSX.")
            elif use_config_path and data_source_path and os.path.isfile(data_source_path): # Check if file exists
                proceed_with_import = True
            elif not use_config_path and uploaded_files:
                proceed_with_import = True
            else:
                st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª(—ã) –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –∏–ª–∏ —É–∫–∞–∂–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –ø—É—Ç—å.")

            if proceed_with_import:
                with st.spinner(f"–ò–º–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö..."):
                    all_dfs = []
                    raw_read_params = schema_for_selected_report.get("read_params", {})
                    
                    # Create a mutable copy of read_params for pandas
                    pd_read_params = raw_read_params.copy()
                    
                    # Extract custom parameters and remove them from what's passed to pandas
                    skip_rows_after_header_count = pd_read_params.pop('skip_rows_after_header', 0)
                    pd_read_params.pop('data_starts_on_row', None) # Remove old param if present

                    if file_type == "google_sheets":
                        try:
                            current_df = read_google_sheets_as_dataframe(data_source_path)
                            if current_df is not None:
                                all_dfs.append(current_df)
                            else:
                                st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ Google Sheets.")
                        except Exception as e_sheets:
                            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ Google Sheets: {e_sheets}")
                    
                    elif file_type == "folder_xlsx":
                        try:
                            xlsx_files_found = False
                            for f_name in os.listdir(data_source_path):
                                if f_name.endswith(".xlsx"):
                                    xlsx_files_found = True
                                    file_path = os.path.join(data_source_path, f_name)
                                    try:
                                        current_df = pd.read_excel(file_path, engine='openpyxl', **pd_read_params)
                                        if skip_rows_after_header_count > 0:
                                            current_df = current_df.iloc[skip_rows_after_header_count:].reset_index(drop=True)
                                        all_dfs.append(current_df)
                                    except Exception as e_read:
                                        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {f_name}: {e_read}")
                            if not xlsx_files_found:
                                st.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ XLSX —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ {data_source_path}.")        
                            elif not all_dfs and xlsx_files_found : # Files were there, but all failed to read or were empty
                                st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ XLSX —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ {data_source_path}, –ª–∏–±–æ —Ñ–∞–π–ª—ã –ø—É—Å—Ç—ã.")

                        except Exception as e_folder:
                             st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ—Å—Ç—É–ø–µ –∫ –ø–∞–ø–∫–µ {data_source_path}: {e_folder}")
                    
                    elif use_config_path and data_source_path: # Using path from config
                        try:
                            if schema_for_selected_report.get("file_type") == "csv":
                                current_df = pd.read_csv(data_source_path, **pd_read_params)
                                # CSVs unlikely to have skip_rows_after_header, but if schema allows, it would apply here too
                                # However, for CSV, skiprows parameter is more direct if needed.
                                # For now, assuming skip_rows_after_header is excel-specific logic in this context.
                            elif schema_for_selected_report.get("file_type") == "xlsx":
                                # Add safety check for large integers in Excel files
                                safe_read_params = pd_read_params.copy()
                                
                                # If no dtype specified, add default string types for known problematic columns
                                if 'dtype' not in safe_read_params:
                                    safe_read_params['dtype'] = {}
                                
                                # Add string types for columns that might contain large integers
                                if selected_report_key == "oz_barcodes":
                                    safe_read_params['dtype'].update({
                                        '–ê—Ä—Ç–∏–∫—É–ª': 'str',
                                        'Ozon Product ID': 'str',
                                        '–®—Ç—Ä–∏—Ö–∫–æ–¥': 'str'
                                    })
                                elif "oz_" in selected_report_key:
                                    # For other Ozon tables, protect SKU and Product ID columns
                                    safe_read_params['dtype'].update({
                                        'OZON id': 'str',
                                        'SKU': 'str',
                                        'Ozon Product ID': 'str'
                                    })
                                elif "wb_" in selected_report_key:
                                    # For Wildberries tables, protect SKU columns
                                    safe_read_params['dtype'].update({
                                        '–ê—Ä—Ç–∏–∫—É–ª WB': 'str'
                                    })
                                
                                st.info(f"üìñ –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –±–æ–ª—å—à–∏—Ö —á–∏—Å–µ–ª: {os.path.basename(data_source_path)}")
                                current_df = pd.read_excel(data_source_path, engine='openpyxl', **safe_read_params)
                                
                                if skip_rows_after_header_count > 0:
                                    current_df = current_df.iloc[skip_rows_after_header_count:].reset_index(drop=True)
                                    
                                st.success(f"‚úÖ –§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–Ω: {len(current_df)} —Å—Ç—Ä–æ–∫")
                            else:
                                st.error(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞ –¥–ª—è —á—Ç–µ–Ω–∏—è: {schema_for_selected_report.get('file_type')}")
                                current_df = None # Ensure it's None
                            if current_df is not None:
                                all_dfs.append(current_df)
                        except FileNotFoundError:
                            st.error(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –ø—É—Ç–∏: {data_source_path}")
                        except Exception as e_read:
                            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {os.path.basename(data_source_path)}: {e_read}")
                            st.error(f"–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –æ—à–∏–±–∫–∏: {str(e_read)}")
                            # Try to provide helpful suggestions
                            if "Value out of range" in str(e_read) or "overflow" in str(e_read).lower():
                                st.warning("üí° –í–æ–∑–º–æ–∂–Ω–∞—è –ø—Ä–∏—á–∏–Ω–∞: —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–µ —á–∏—Å–ª–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:")
                                st.write("1. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤ —Å—Ö–µ–º–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö")
                                st.write("2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –≤—Å–µ SKU –∏ Product ID –∫–æ–ª–æ–Ω–∫–∏ —á–∏—Ç–∞—é—Ç—Å—è –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏")
                                st.write("3. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ BIGINT –º–∏–≥—Ä–∞—Ü–∏–∏")

                    else: # Single file upload (or list if accept_multiple_files was True)
                        files_to_process = uploaded_files
                        if files_to_process and not isinstance(files_to_process, list):
                            files_to_process = [files_to_process] # Make it a list for uniform processing
                        
                        if files_to_process:
                            for uploaded_file_obj in files_to_process: # Renamed to avoid conflict
                                if uploaded_file_obj is not None:
                                    try:
                                        if schema_for_selected_report.get("file_type") == "csv":
                                            current_df = pd.read_csv(uploaded_file_obj, **pd_read_params)
                                            # Similar to above, skip_rows_after_header is assumed excel-specific here
                                        elif schema_for_selected_report.get("file_type") == "xlsx":
                                            # Add safety check for large integers in uploaded Excel files
                                            safe_read_params = pd_read_params.copy()
                                            
                                            # If no dtype specified, add default string types for known problematic columns
                                            if 'dtype' not in safe_read_params:
                                                safe_read_params['dtype'] = {}
                                            
                                            # Add string types for columns that might contain large integers
                                            if selected_report_key == "oz_barcodes":
                                                safe_read_params['dtype'].update({
                                                    '–ê—Ä—Ç–∏–∫—É–ª': 'str',
                                                    'Ozon Product ID': 'str',
                                                    '–®—Ç—Ä–∏—Ö–∫–æ–¥': 'str'
                                                })
                                            elif "oz_" in selected_report_key:
                                                # For other Ozon tables, protect SKU and Product ID columns
                                                safe_read_params['dtype'].update({
                                                    'OZON id': 'str',
                                                    'SKU': 'str',
                                                    'Ozon Product ID': 'str'
                                                })
                                            elif "wb_" in selected_report_key:
                                                # For Wildberries tables, protect SKU columns
                                                safe_read_params['dtype'].update({
                                                    '–ê—Ä—Ç–∏–∫—É–ª WB': 'str'
                                                })
                                            
                                            st.info(f"üìñ –ß—Ç–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –±–æ–ª—å—à–∏—Ö —á–∏—Å–µ–ª: {uploaded_file_obj.name}")
                                            current_df = pd.read_excel(uploaded_file_obj, engine='openpyxl', **safe_read_params)
                                            
                                            if skip_rows_after_header_count > 0:
                                                current_df = current_df.iloc[skip_rows_after_header_count:].reset_index(drop=True)
                                                
                                            st.success(f"‚úÖ –§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–Ω: {len(current_df)} —Å—Ç—Ä–æ–∫")
                                        else: 
                                            st.error(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞ –¥–ª—è —á—Ç–µ–Ω–∏—è: {schema_for_selected_report.get('file_type')}")
                                            continue
                                        all_dfs.append(current_df)
                                    except Exception as e_read:
                                        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {uploaded_file_obj.name}: {e_read}")
                                        st.error(f"–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –æ—à–∏–±–∫–∏: {str(e_read)}")
                                        # Try to provide helpful suggestions
                                        if "Value out of range" in str(e_read) or "overflow" in str(e_read).lower():
                                            st.warning("üí° –í–æ–∑–º–æ–∂–Ω–∞—è –ø—Ä–∏—á–∏–Ω–∞: —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–µ —á–∏—Å–ª–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:")
                                            st.write("1. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤ —Å—Ö–µ–º–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö")
                                            st.write("2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –≤—Å–µ SKU –∏ Product ID –∫–æ–ª–æ–Ω–∫–∏ —á–∏—Ç–∞—é—Ç—Å—è –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏")
                                            st.write("3. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ BIGINT –º–∏–≥—Ä–∞—Ü–∏–∏")
                        else: # This case should be caught by proceed_with_import logic, but as a fallback
                            st.warning("–§–∞–π–ª—ã –Ω–µ –±—ã–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")
                    
                    if all_dfs:
                        df_combined = pd.concat(all_dfs, ignore_index=True)
                        if not df_combined.empty:
                            target_table_name = selected_report_key # The key is the table name
                            
                            st.write(f"–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –ø–µ—Ä–≤—ã—Ö 5 —Å—Ç—Ä–æ–∫ –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –≤ —Ç–∞–±–ª–∏—Ü—É '{target_table_name}':")
                            st.dataframe(df_combined.head())

                            success, count, error_message = import_data_from_dataframe(
                                db_conn,
                                df_combined,
                                target_table_name
                            )
                            if success:
                                st.success(f"–£—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {count} –∑–∞–ø–∏—Å–µ–π –≤ —Ç–∞–±–ª–∏—Ü—É '{target_table_name}'.")
                                # Optionally, clear uploader or refresh stats
                            else:
                                st.error(f"–û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –≤ —Ç–∞–±–ª–∏—Ü—É '{target_table_name}': {error_message}")
                        else:
                            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤.")
                    else:
                        st.warning("–ù–µ –≤—ã–±—Ä–∞–Ω—ã —Ñ–∞–π–ª—ã –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –∏–ª–∏ —Ñ–∞–π–ª—ã –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –¥–∞–Ω–Ω—ã—Ö.")
            else:
                st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª(—ã) –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –∏–ª–∏ —É–∫–∞–∂–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –ø—É—Ç—å.")

        # Show special information for punta_table
        if selected_report_key == "punta_table":
            st.info("""
            üîÑ **–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ punta_table**
            
            –≠—Ç–∞ —Ç–∞–±–ª–∏—Ü–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –≤–∞—à–µ–≥–æ Google Sheets –¥–æ–∫—É–º–µ–Ω—Ç–∞:
            ‚Ä¢ **–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è —Å—Ö–µ–º–∞**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≤ Google Sheets
            ‚Ä¢ **–ì–∏–±–∫–æ—Å—Ç—å**: –ú–æ–∂–µ—Ç–µ –¥–æ–±–∞–≤–ª—è—Ç—å –∏–ª–∏ –∏–∑–º–µ–Ω—è—Ç—å –∫–æ–ª–æ–Ω–∫–∏ –≤ Google Sheets –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–æ–¥–∞
            ‚Ä¢ **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ wb_sku**: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç wb_sku –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            ‚Ä¢ **–ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞**: –ü—Ä–∏ –∫–∞–∂–¥–æ–º –∏–º–ø–æ—Ä—Ç–µ —Ç–∞–±–ª–∏—Ü–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ—Ç—Å—è —Å –Ω–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
            
            **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
            - –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ Google Sheets –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∫–æ–ª–æ–Ω–æ–∫
            - –ö–æ–ª–æ–Ω–∫–∞ 'wb_sku' –±—É–¥–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –∫–∞–∫ –∫–ª—é—á–µ–≤–æ–µ –ø–æ–ª–µ
            - –í—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã –≤ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á–µ—Ç–∞—Ö –∫–∞–∫ PUNTA_–Ω–∞–∑–≤–∞–Ω–∏–µ_–∫–æ–ª–æ–Ω–∫–∏
            """)
    else:
        st.error(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ö–µ–º—ã –¥–ª—è –æ—Ç—á–µ—Ç–∞: {selected_report_key}")

st.sidebar.markdown("---")
if st.sidebar.button("–û–±–Ω–æ–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫ –æ—Ç—á–µ—Ç–æ–≤"):
    st.rerun()

# Add any other UI elements or logic as needed
# Example: Displaying last import status or logs
# with st.expander("–õ–æ–≥–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏–º–ø–æ—Ä—Ç–∞"):
#    st.json(st.session_state.get("last_import_logs", {})) 